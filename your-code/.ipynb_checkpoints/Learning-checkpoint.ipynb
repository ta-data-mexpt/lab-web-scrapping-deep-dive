{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Deep Dive\n",
    "\n",
    "\n",
    "Lesson Goals\n",
    "\n",
    "    Learn how to handle different response status codes.\n",
    "    Learn how to handle request errors.\n",
    "    Learn how to define the user agent string in requests.\n",
    "    Learn how to make asynchronous requests.\n",
    "    Learn how to limit asynchronous requests rate.\n",
    "    Understanding robots.txt\n",
    "\n",
    "Introduction\n",
    "\n",
    "Now that you have had the basic knowledge about and hands-on experience with web scraping, it's the time to dive deep in. WWW is the biggest man-made data repository in the history. But the format, quality, and stability of the data sources (i.e. websites) are extremely unpredictable. Therefore, you have to learn how to deal with those issues in this imperfect world. In addition, there are many human-imposed rules and conventions we need to follow and respect because we are data analysts not Internet spies.\n",
    "\n",
    "In this lesson, you will learn about the common challenges and pitfalls in web scraping as well as what techniques you can use in order to get the job done. After mastering those techniques, you will be prepared to scrape the modern web in reality.\n",
    "Handling Web Response Status Codes\n",
    "\n",
    "In the API Deep Dive lesson, you have learned about the API response status codes. In web scraping you will encounter the same status codes because those status codes are designed for the whole Internet. The request Python library you have learned previously allows you to detect the web response status code so that you can handle different types of responses accordingly.\n",
    "\n",
    "Try the command below in your Python console:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "403\n",
      "[<Response [301]>]\n",
      "301\n",
      "<HTML><HEAD><meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\">\n",
      "<TITLE>301 Moved</TITLE></HEAD><BODY>\n",
      "<H1>301 Moved</H1>\n",
      "The document has moved\n",
      "<A HREF=\"http://www.google.com/\">here</A>.\n",
      "</BODY></HTML>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "r1 = requests.get('https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/documents/the-html5-breakfast-site.html')\n",
    "print(r1.status_code)\n",
    "r2 = requests.get('https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/documents/forbidden')\n",
    "print(r2.status_code)\n",
    "r3 = requests.get('http://google.com')\n",
    "print(r3.history)\n",
    "\n",
    "print(r3.history[0].status_code)\n",
    "\n",
    "print(r3.history[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, the requests library allows you to check whether your web request is successful (i.e. with 20x status codes) or not (i.e. with 40x, or 50x status codes). It's a good idea to check the status code before you parse the response text. Once you receive the response from the requests.get method, do something like below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request was successful\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url='http://google.com'\n",
    "\n",
    "r = requests.get(url) \n",
    "if r.status_code < 300:\n",
    "    print('request was successful')\n",
    "elif r.status_code >= 400 and r.status_code < 500:\n",
    "    print('request failed because the resource either does not exist or is forbidden')\n",
    "else:\n",
    "    print('request failed because the response server encountered an error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted that the requests lib automatically makes additional requests to the redirected URL if the web resource is moved (i.e. 30x status codes). Even if a moved web resource redirects once again, the requests lib will track it all the way down until it receives success or failure as long as the number of redirects does not exceed the redirect limit (default 30). You can choose to disallow redirects by using requests.get(url, allow_redirects=False) so that requests will not track down the redirected URL. Or you can choose to reduce the max redirects allowed by using requests.get(url, max_redirects=3) so as to avoid endless redirects or save time in making requests.\n",
    "\n",
    "\n",
    "\n",
    "# Handling Request Errors\n",
    "\n",
    "Sometimes requests cannot complete its transaction and you will need to catch the error then handle them accordingly. Some of the common errors you may encounter include:\n",
    "Exceeing Max Redirects\n",
    "\n",
    "As previously mentioned, there is a max redirects number by default (30) which you can override with max_redirects. If the number of redirects exceeds the limit, requests will throw a TooManyRedirects exception.\n",
    "\n",
    "\n",
    "Timeout\n",
    "\n",
    "Sometimes a remote server is not responsive either because requests cannot connect to the intended web resource or because the remote server does not send back the promised data. If that happens, requests will typically wait for a long period of time until the connection is closed by the remote server then throw a ConnectTimeout exception. This is a big waste of time because most modern websites respond to web requests within a couple of seconds. Therefore, it's a desirable approach to supply a timeout argument to requests to limit the amount of wait time.\n",
    "\n",
    "\n",
    "SSL Certificate Error\n",
    "\n",
    "Secure Sockets Layer (SSL) is a standard security protocol for establishing encrypted links between a web server and a browser in an online communication. To know if the browser connection to a website is encrypted or not, simply check if the URL starts with HTTP or HTTPS. The successor of SSL is Transport Layer Security (TLS) which is strongly promoted by tech giants such as Google. For example, Chrome has been marking HTTP webpage as insecure and discouraging users from connecting to those webpages. The efforts from these tech giants are effectively pushing the whole Internet towards adopting SSL/TLS because if a website doesn't adopt the standard it will lose users.\n",
    "\n",
    "However, if a website wants to use SSL/TLS, it has to purchase a special certificate from certificate vendors and configure the web server properly in order for the certificate to be functional. If the SSL/TLS certificate is not installed properly or it has expired (purchased certificate has to be renewed every two years), modern browsers such as Chrome will indicate the problem to the users. The requests lib, similarly, will throw an exception if it detects the SSL certificate is problematic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    r = requests.get(url, timeout=10)\n",
    "except requests.exceptions.Timeout:\n",
    "    print ('a')\n",
    "    # timeout error... do something\n",
    "#except requests.exceptions.TooManyRedirects:\n",
    "    # redirect error... do something\n",
    "#except requests.exceptions.SSLError:\n",
    "    # ssl error... do something\n",
    "#except requests.exceptions.RequestException as e:\n",
    "    # other unknown errors... do something"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Agent\n",
    "\n",
    "When a browser makes HTTP/HTTPS requests, there is a special field in the request header called User-Agent that identifies the browser and operating system from which the request is made. Some websites send out different responses to the requests made with different user agents for reasons such as:\n",
    "\n",
    "    They want to avoid bugs of the website that only occur in certain browsers or browser versions.\n",
    "\n",
    "    They want to personalize user experience (e.g. desktop vs mobile, small vs big screen, etc.) by sending different data.\n",
    "\n",
    "    They don't want to send the same responses to search engines as to human users.\n",
    "\n",
    "Not all websites differentiate user agents and if they do there is often a good reason. You don't need to worry too much about what user agents to use when you request web data. But it's helpful to have this knowledge. In case you decide to fool the website by pretending to be a certain browser, use the approach below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!doctype html><html itemscope=\"\" itemtype=\"http://schema.org/WebPage\" lang=\"es\"><head><meta charset=\"UTF-8\"><meta content=\"origin\" name=\"referrer\"><meta content=\"Google.es permite acceder a la informaci\\xc3\\xb3n mundial en castellano, catal\\xc3\\xa1n, gallego, euskara e ingl\\xc3\\xa9s.\" name=\"description\"><meta content=\"noodp\" name=\"robots\"><meta content=\"/images/branding/googleg/1x/googleg_standard_color_128dp.png\" itemprop=\"image\"><meta content=\"origin\" name=\"referrer\"><title>Google</title><script nonce=\"+ZDEYVRC7WIBi72wYPh1xA==\">(function(){window.google={kEI:\\'Qm4jXZ2lDNeLjLsPlMkP\\',kEXPI:\\'31\\',authuser:0,kscs:'\n"
     ]
    }
   ],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'}\n",
    "response = requests.get(url, headers=headers)\n",
    "print(response.content[0:600])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the uer agent string above, the request pretends to be from Chrome browser v71.0.3578.98 in macOS 10.14.2 (Mojave). To see what the user agent string look like in other browsers/OS, check out this documentation.\n",
    "Making Asynchronous Requests\n",
    "\n",
    "When you make massive requests to websites, it can be extremely time-consuming. To complete your request faster, you can take advantage of the async module of requests. Here's how ( you'll need to first install asyncio with pip before you can try the following code ): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "This event loop is already running",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-01a46c561b1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mloop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_event_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/asyncio/base_events.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_done_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_run_until_complete_cb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_forever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnew_task\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancelled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/asyncio/base_events.py\u001b[0m in \u001b[0;36mrun_forever\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This event loop is already running'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             raise RuntimeError(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: This event loop is already running"
     ]
    }
   ],
   "source": [
    "import asyncio, requests\n",
    "\n",
    "urls = [\n",
    "    'https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/documents/breakfast.jpg',\n",
    "    'https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/documents/forbidden',\n",
    "    'https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/documents/the-html5-breakfast-site.html'\n",
    "]\n",
    "\n",
    "async def main():\n",
    "    loop = asyncio.get_event_loop()\n",
    "    futures = [loop.run_in_executor(None, requests.get, url) for url in urls]\n",
    "    for response in await asyncio.gather(*futures):\n",
    "        print(response.status_code)\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Throttling and Rate Limiting\n",
    "\n",
    "In modern websites especially those having massive users, throttling and/or rate limiting is often enforced so that a certain person cannot make too frequent API/web requests to the website. These approaches are not targeting at regular human users but rather search engine bots and especially hackers. With throttling, the same requester (usually judged by IP address or account) cannot make more requests than the limit allowed within a certain period of time (e.g. 10,000 requests/day). If the limit is exceeded, the requester will receive an error or simply no response. With rate limiting, a requester must control the frequency of the requests under a certain threshold (e.g. 10 requests/second). When you test your web scraping scripts, if you receive a lot of errors in your responses, it does not necessarily mean the web resources are invlaid. It may because the websites you make requests to are throttling or limiting your requests.\n",
    "\n",
    "The throttling thresholds and wait peroids differ from website to website. In order to know whether you may be throttled by exceeding the limit, you need to check out the user agreement of the website that you make requests to.\n",
    "\n",
    "To control the request rate of your scripts, you can wait a period of time after making each request by calling time.sleep(). For example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "403\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "import requests, time\n",
    "\n",
    "urls = [\n",
    "    'https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/documents/breakfast.jpg',\n",
    "    'https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/documents/forbidden',\n",
    "    'https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/documents/the-html5-breakfast-site.html'\n",
    "]\n",
    "\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    print(response.status_code)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
